{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping and Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Data Dictionary](#0)\n",
    "- [Web Scraping](#1)\n",
    "\t- [Part 1a: Scraping from the browsing pages](#2)\n",
    "\t- [Part 1b: Cleaning the HTML](#3)\n",
    "\t- [Part 2a: Scraping from individual record pages](#4)\n",
    "\t- [part 2b: Cleaning the HTML](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ppl_have: People in discogs community who own the record \n",
    "- ppl_want: People in community who want the record\n",
    "- weighted ratio: square root( (people who want record)^2/ people who have record )\n",
    "- rated: average star rating (1-5 system)\n",
    "- genre: The main category/s a record falls under. (just 1 value for most records)\n",
    "- style: Subcategory of genre, defining the type of music in more detail. (most records have a list of styles)\n",
    "- media_condition: the condition which the vinyl is in\n",
    "- sleeve_condition: the condition of the case\n",
    "- age: (2019 - the year of production)\n",
    "- track_no: number of tracks on the record\n",
    "- format: the type of record (album, 12\", LP, Single etc)\n",
    "- 12\": a format of vinyl that has thicker grooves for higher sound quality, but fits less music\n",
    "- ships_from: country the record is selling from\n",
    "- country_origin: country the record was produced in\n",
    "- has_b_side: binary response for music on both sides of the record. (it's also possible that the listing contains more than 1 album)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleansing and data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import ast as ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "#web scraping\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## Part 1: Scraping the browsing pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping 400 pages of 250 records per page. Aiming to arrive with 100,000 records, though many will be duplicates\n",
    "# or have missing values\n",
    "\n",
    "search_urls=[]\n",
    "\n",
    "for i in range(1,400):\n",
    "    search_urls.append('https://www.website.com/sell/list?sort=listed%2Cdesc&limit=250&format=Vinyl&page={}'.format(i))\n",
    "search_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_thumb=[]     #Section of web page containing; Ratings, Users have, Users want\n",
    "soup_details=[]   #Section of web page containing; Name, Sleeve, Media condition, (Record URLs contained within this block of HTML)\n",
    "soup_seller=[]    #Section of web page containing; Seller, Rating, Ratings, Ships From\n",
    "soup_money=[]     #Section of web page containing; price\n",
    "\n",
    "\n",
    "#  Using Selenium to automate a google chrome session, which will loop through webpages, and pull all of the HTML code\n",
    "\n",
    "#  Then using Beautiful soup to process the HTML. Identifying which interval in the webpage is a new record \n",
    "#  and dividing the HTML into sections for each record, making for easier processing later on.\n",
    "\n",
    "driver = webdriver.Chrome('/Users/josephstern/Downloads/chromedriver')\n",
    "for i in tqdm(search_urls):\n",
    "\n",
    "    driver.get(i)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    for info in soup.find_all('td', attrs={'class':'item_picture as_float'}):\n",
    "        try:\n",
    "            soup_thumb.append(info)\n",
    "        except:\n",
    "            soup_thumb.append(np.nan)\n",
    "            \n",
    "    for info in soup.find_all('td', attrs={'class':'item_description'}):\n",
    "        try:\n",
    "            soup_details.append(info)\n",
    "        except:\n",
    "            soup_details.append(np.nan)\n",
    "    \n",
    "    for info in soup.find_all('td', attrs={'class':'seller_info'}):\n",
    "        try:\n",
    "            soup_seller.append(info)\n",
    "        except:\n",
    "            soup_seller.append(np.nan)\n",
    "            \n",
    "    for info in soup.find_all('td', attrs={'class':'item_price hide_mobile'}):\n",
    "        try:\n",
    "            soup_money.append(info.find('span', attrs={'class':'price'}).text.strip())\n",
    "        except:\n",
    "            soup_money.append(np.nan)\n",
    "            \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using brute force methods to locate the data within each block of HTML.\n",
    "# despite the absolute chaos within the HTML, it is consistent for almost every record\n",
    "\n",
    "#from soup thumb\n",
    "rated=[]\n",
    "have=[]\n",
    "want=[]\n",
    "\n",
    "for thing in soup_thumb:\n",
    "    if 'Rated:' in thing.text:\n",
    "        rated.append(float(re.search(r\"Rated: (.*)\\n\\n\\n\\n\\n\",thing.text).group(1)))\n",
    "    else:\n",
    "        rated.append(np.nan)\n",
    "        \n",
    "    if 'have' in thing.text:\n",
    "        have.append(int(re.search(r\"\\n(.*)have\",thing.text).group(1)))\n",
    "    else:\n",
    "        have.append(np.nan)\n",
    "\n",
    "    if 'want' in thing.text:\n",
    "        want.append(int(re.search(r\"\\n(.*)want\",thing.text).group(1)))\n",
    "    else:\n",
    "        want.append(np.nan)\n",
    "\n",
    "#from soup details        \n",
    "name=[]\n",
    "media_cond=[]\n",
    "sleeve_cond=[]\n",
    "label=[]\n",
    "\n",
    "for thing in soup_details:\n",
    "    if '\\n' in thing.text:\n",
    "        name.append(str(re.search(r\"\\n\\n(.*)\\n\\n\\n\",thing.text).group(1)))\n",
    "    else:\n",
    "        name.append(np.nan)\n",
    "        \n",
    "    if 'Label:' in thing.text:\n",
    "        label.append(str(re.search(r\"Label:(.*)\\n\",thing.text).group(1)))\n",
    "    else:\n",
    "        label.append(np.nan)\n",
    "        \n",
    "    if 'Media:' in thing.text:\n",
    "        media_cond.append(str(re.search(r\"Media:\\n\\n(.*)\\n\\n\",thing.text).group(1).strip()))\n",
    "    else:\n",
    "        media_cond.append(np.nan)\n",
    "\n",
    "    if 'Sleeve:' in thing.text:\n",
    "        sleeve_cond.append(str(re.search(r\"Sleeve:\\n(.*)\\n\\n\\n\\n\",thing.text).group(1).strip()))\n",
    "    else:\n",
    "        sleeve_cond.append(np.nan)\n",
    "\n",
    "#from soup seller\n",
    "seller=[]\n",
    "av_rating=[]\n",
    "ratings=[]\n",
    "ships_from=[]\n",
    "\n",
    "for thing in soup_seller:\n",
    "    if 'Seller:' in thing.text:\n",
    "        seller.append(str(re.search(r\"Seller:\\n(.*)\\n\",thing.text).group(1)))\n",
    "    else:\n",
    "        seller.append(np.nan)\n",
    "        \n",
    "    if '%' in thing.text:\n",
    "        av_rating.append(float(re.search(r\"\\n\\n\\n\\n(.*)%\",thing.text).group(1).strip()))\n",
    "    else:\n",
    "        av_rating.append(np.nan)\n",
    "        \n",
    "    if 'ratings' in thing.text:\n",
    "        ratings.append(int(re.search(r\"\\n(.*)ratings\",thing.text).group(1).strip().replace(',','')))\n",
    "    else:\n",
    "        ratings.append(np.nan)\n",
    "        \n",
    "    if 'Ships From:' in thing.text:\n",
    "        ships_from.append(str(re.search(r\"From:(.*)\\n\\n\\n\\n\\n\",thing.text).group(1).strip()))\n",
    "    else:\n",
    "        ships_from.append(np.nan)\n",
    "\n",
    "#checks\n",
    "check_list=[ratings, have, want, name, label, media_cond, sleeve_cond, seller, av_rating, ratings, ships_from]\n",
    "\n",
    "for item in check_list:\n",
    "    print(len(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "### Cleaning Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing unnecessary characters and converting currencies\n",
    "\n",
    "#name\n",
    "artist=[re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).replace('*','').split(' - ')[0] for x in name]\n",
    "album=[re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).replace('*','').split(' - ')[1] for x in name]\n",
    "\n",
    "#label\n",
    "label_=[re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x) for x in label]\n",
    "\n",
    "#price\n",
    "currency_3d=['CA$','CHF','SEK','NZ$','MX$','ZAR']\n",
    "currency_2d=['A$','R$']\n",
    "\n",
    "gbp=[]\n",
    "currency=[]\n",
    "value=[]\n",
    "\n",
    "for price in soup_money:\n",
    "    try:\n",
    "        if price[:3] in currency_3d:\n",
    "            currency.append(price[:3])\n",
    "            value.append(float(price[3:].replace(',','.')))\n",
    "        elif price[:2] in currency_2d:\n",
    "            currency.append(price[:2])\n",
    "            value.append(float(price[2:].replace(',','.')))                \n",
    "        else:\n",
    "            currency.append(price[0])\n",
    "            value.append(float(price[1:].replace(',','.')))\n",
    "    except:\n",
    "        value.append(np.nan)\n",
    "\n",
    "for price in list(zip(currency,value)):\n",
    "    if price[0]=='$':\n",
    "        gbp.append(round(price[1]*0.79,2))\n",
    "    elif price[0]=='€':\n",
    "        gbp.append(round(price[1]*0.9,2))\n",
    "    elif price[0]=='CA$':\n",
    "        gbp.append(round(price[1]*0.58,2))\n",
    "    elif price[0]=='A$':\n",
    "        gbp.append(round(price[1]*0.56,2))\n",
    "    elif price[0]=='CHF':\n",
    "        gbp.append(round(price[1]*0.8,2))\n",
    "    elif price[0]=='¥':\n",
    "        gbp.append(round(price[1]*0.0071,2))\n",
    "    elif price[0]=='SEK':\n",
    "        gbp.append(round(price[1]*0.088,2)) \n",
    "    elif price[0]=='R$':\n",
    "        gbp.append(round(price[1]*0.21,2))\n",
    "    elif price[0]=='NZ$':\n",
    "        gbp.append(round(price[1]*0.53,2)) \n",
    "    elif price[0]=='MX$':\n",
    "        gbp.append(round(price[1]*0.04,2))\n",
    "    elif price[0]=='ZAR':\n",
    "        gbp.append(round(price[1]*0.55,2)) \n",
    "    elif price[0]=='£':\n",
    "        gbp.append(price[1])\n",
    "    else:\n",
    "        gbp.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL list for 2nd part of the web scraping. essentially creating a list of 100,000 web pages to scrape at individual\n",
    "# record level\n",
    "url_links=[]\n",
    "for i in range(len(soup_details)):\n",
    "    url_links.append('https://www.website.com'+str(soup_details[i].find_all('a')[0]).split(' ')[3].split('\"')[1])\n",
    "    \n",
    "print(len(url_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting first round of scraping results into an organised dataframe\n",
    "df1=pd.DataFrame({'currency':currency,\n",
    "                  'pound_value':gbp,\n",
    "                  'artist':artist,\n",
    "                  'album':album,\n",
    "                  'rated':rated,\n",
    "                  'ppl_have':have,\n",
    "                 'ppl_want':want,\n",
    "                 'wh_ratio':ratio,\n",
    "                 'weighted_ratio':ratio_w,\n",
    "                 'vinyl_condition':media_cond,\n",
    "                 'sleeve_condition':sleeve_cond,\n",
    "                 'label':label,\n",
    "                 'seller':seller,\n",
    "                 'seller_rating':av_rating,\n",
    "                 'rated_sales':ratings,\n",
    "                 'ships_from':ships_from,\n",
    "                 'url':url_links\n",
    "                 })\n",
    "\n",
    "#test=df1.artist+df1.album\n",
    "#print('duplicates:',1-len(pd.DataFrame(test)[0].unique())/len(df1))\n",
    "\n",
    "#Dropping duplicate records\n",
    "df1=df1.drop_duplicates(subset=['artist', 'album'], keep='first')\n",
    "\n",
    "#Dealing with Missing Values\n",
    "df1['ppl_have']=df1['ppl_have'].fillna(0)\n",
    "df1['ppl_want']=df1['ppl_want'].fillna(0)\n",
    "df1['weighted_ratio']=df1['weighted_ratio'].fillna(0)\n",
    "df1['sleeve_condition']=df1['sleeve_condition'].fillna('Very Good Plus (VG+)')  \n",
    "df1['seller_rating']=df1['seller_rating'].fillna(df1.seller_rating.mean())\n",
    "df1['rated_sales']=df1['rated_sales'].fillna(0)\n",
    "\n",
    "#Just dropping records with no rating. Bad idea to fill with mean as it is quite influential later on\n",
    "df1=df1.dropna()\n",
    "\n",
    "#Exporting to CSV\n",
    "df1.to_csv('my path/df1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "### Part 2: Scraping individual record pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('my path/df1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_details2=[]\n",
    "soup_tracks=[]\n",
    "url=[]    #will be needed for inner joining, especially if the lengths don't add up\n",
    "x=0   #I put this in place as a sort of bookmark, so i could scrape ~10,000 records at a time \n",
    "      #and check my results before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a bot to interrogate each page individually. This is the long part. \n",
    "# scraping 8 pages at a time in warp speed then sleeping for 5 seconds.\n",
    "# Most efficient way I could find that avoids being blocked by website.\n",
    "\n",
    "driver = webdriver.Chrome('/Users/josephstern/Downloads/chromedriver')\n",
    "#driver = webdriver.Firefox(executable_path=r'/Users/josephstern/Downloads/geckodriver') #Firefox was slower\n",
    "\n",
    "for j in tqdm(range(12500)):\n",
    "    time.sleep(5)\n",
    "    for i in df1.url[x:(x+8)]:\n",
    "        driver.get(i)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html)\n",
    "\n",
    "        for info in soup.find_all('div', attrs={'class':'profile'}):\n",
    "            try:\n",
    "                soup_details2.append(info)\n",
    "                url.append(i)\n",
    "            except:\n",
    "                soup_details2.append(np.nan)\n",
    "                url.append(i)\n",
    "\n",
    "        for tracks in soup.find_all('table', attrs={'class':'playlist'}):\n",
    "            try:\n",
    "                soup_tracks.append(tracks)\n",
    "            except:\n",
    "                soup_tracks.append(np.nan)\n",
    "        x+=1\n",
    "        \n",
    "driver.quit()\n",
    "\n",
    "#checking how many pages were browsed and that the lists add up\n",
    "x,len(soup_details2),len(soup_tracks),len(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same steps as before, extracting details that can only be seen on the individual record pages. \n",
    "\n",
    "format_=[]\n",
    "country=[]\n",
    "year=[]\n",
    "genre=[]\n",
    "style=[]\n",
    "\n",
    "#Scraped from the main block of details\n",
    "for thing in soup_details2:\n",
    "\n",
    "    if 'Format:' in thing.text:\n",
    "        format_.append(re.search(r\"Format:(.*)\\n\",thing.text.replace(' ','').replace('\\n\\n',' ')).group(1).strip())\n",
    "    else:\n",
    "        format_.append(np.nan)\n",
    "\n",
    "    if 'Country:' in thing.text:\n",
    "        country.append(re.search(r\"Country:(.*)\\n\",thing.text.replace('\\n\\n',' ')).group(1).strip())\n",
    "    else:\n",
    "        country.append(np.nan)\n",
    "\n",
    "    if 'Released:' in thing.text:\n",
    "        year.append(re.search(r\"Released:(.*)\\n\",thing.text.replace('\\n\\n',' ')).group(1).strip())\n",
    "    else:\n",
    "        year.append(np.nan)\n",
    "\n",
    "    if 'Genre:' in thing.text:\n",
    "        try:\n",
    "            genre.append(re.search(r\"Genre: \\n(.*)\\n\",thing.text.replace(' ','').replace('\\n\\n',' ')).group(1).strip())\n",
    "        except:\n",
    "            genre.append(np.nan)\n",
    "\n",
    "    if 'Style:' in thing.text:\n",
    "        style.append(thing.text.replace(' ','').replace('\\n\\n',' ').split('\\n')[-1].strip())\n",
    "    else:\n",
    "        style.append(np.nan)\n",
    "\n",
    "        \n",
    "#Some tracks are split by 6 break spaces, some are split by 3.      \n",
    "\n",
    "tracks=[]\n",
    "for thing in soup_tracks:\n",
    "    try:\n",
    "        if '\\n\\n\\n\\n\\n\\n' in thing.text:\n",
    "            tracks.append(len(thing.text.split('\\n\\n\\n\\n\\n\\n'))-1)\n",
    "        elif '\\n\\n\\n' in thing.text:\n",
    "            tracks.append(len(thing.text.split('\\n\\n\\n'))-1)\n",
    "        else:\n",
    "            tracks.append(np.nan)\n",
    "    except:\n",
    "        tracks.append(np.nan)\n",
    "        \n",
    "# Creating a new feature to indicate the presence of a B side.\n",
    "\n",
    "b_side=[]\n",
    "for x in soup_tracks:\n",
    "    if 'B1' in x.text:\n",
    "        b_side.append(1)\n",
    "    else:\n",
    "        b_side.append(0)\n",
    "        \n",
    "\n",
    "#checks\n",
    "check_list=[format_, country, year, genre, style, tracks]\n",
    "\n",
    "for item in check_list:\n",
    "    print(len(item))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "### Cleaning Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA CLEANING\n",
    "\n",
    "#country\n",
    "country_=[np.nan if x=='' else x for x in country]\n",
    "\n",
    "#year\n",
    "year_=[]\n",
    "for x in year:\n",
    "    try:\n",
    "        if x=='':\n",
    "            year_.append(np.nan)\n",
    "        else:\n",
    "            year_.append(int(x[-4:]))\n",
    "    except:\n",
    "        year_.append(np.nan)\n",
    "\n",
    "\n",
    "#style\n",
    "style_=[np.nan if x=='Style:' else x.split(',') for x in style]\n",
    "\n",
    "#genre\n",
    "genre_=[]\n",
    "for x in genre:\n",
    "    try:\n",
    "        if x=='':\n",
    "            genre_.append(np.nan)\n",
    "        else:\n",
    "            genre_.append(x.split(','))\n",
    "    except:\n",
    "        genre_.append(np.nan)\n",
    "\n",
    "\n",
    "#format\n",
    "format_2=[x.split(',') for x in format_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame({'format_':format_2,\n",
    "              'country_origin':country_,\n",
    "              'year':year_,\n",
    "              'genre':genre_,\n",
    "              'style':style_,\n",
    "              'has_b_side':b_side,\n",
    "              'track_no':tracks,\n",
    "              'url':url})\n",
    "\n",
    "df2.to_csv('my path /df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.merge(df1,df2,on='url',how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing Final Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of un-named columns columns\n",
    "df4=pd.concat([df3.iloc[:,2:18], df3.iloc[:,19:26]], axis=1, sort=False)\n",
    "\n",
    "#dropping records with track counts that look wrong- could be mistake as the cleansing criteria was quite loose, \n",
    "    #but also could be massive compilations or something weird \n",
    "df5=df4[df4.track_no<24]\n",
    "\n",
    "#filling blank year with average per genre\n",
    "df5['year'] = df5.groupby(['genre'])['year'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "#dropping all other values\n",
    "df6=df5.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new binarized dataset, for Format, Genre and Style columns.\n",
    "# these columns are lists of lists, so i needed to do something else with them\n",
    "# keeping this seperate from main dataset for now as it'll add ~500 extra columns\n",
    "\n",
    "#stored as objects for some reason, converting back to list.\n",
    "test_format=[ast.literal_eval(x) for x in df6.format_]\n",
    "test_genre=[ast.literal_eval(x) for x in df6.genre]\n",
    "test_style=[ast.literal_eval(x) for x in df6['style']]\n",
    "\n",
    "#FORMATS\n",
    "df = pd.DataFrame({'groups':test_format}, columns=['groups'])\n",
    "s = df['groups']\n",
    "mlb = MultiLabelBinarizer()\n",
    "formats=pd.DataFrame(mlb.fit_transform(s),columns=mlb.classes_, index=df.index)\n",
    "\n",
    "#just keeping the most common formats as there are way too many here\n",
    "df6.format_.value_counts()\n",
    "\n",
    "acceptable_formats=['Vinyl','LP','Album','12\"','45RPM','33⅓RPM',\n",
    "                '7\"','Single','2×','Compilation','EP','Reissue',\n",
    "                'Stereo','Promo','Mono','Gatefold','Maxi-Single','3×']\n",
    "\n",
    "formats=formats[acceptable_formats]\n",
    "\n",
    "\n",
    "#GENRES\n",
    "df = pd.DataFrame({'groups':test_genre}, columns=['groups'])\n",
    "s = df['groups']\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres=pd.DataFrame(mlb.fit_transform(s),columns=mlb.classes_, index=df.index)\n",
    "genre_list=list(genres.columns)\n",
    "\n",
    "#STYLES\n",
    "df = pd.DataFrame({'groups':test_style}, columns=['groups'])\n",
    "s = df['groups']\n",
    "mlb = MultiLabelBinarizer()\n",
    "styles=pd.DataFrame(mlb.fit_transform(s),columns=mlb.classes_, index=df.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols=pd.concat([formats,genres,styles], axis=1, sort=False)\n",
    "binary_cols.to_csv('my path/binary_cols.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping the lists of lists out of my exploratory analysis for now\n",
    "df7=df6.drop(columns=['format_','style','genre'])\n",
    "df7.columns\n",
    "df7.to_csv('/Users/josephstern/Desktop/project_files/df7.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
